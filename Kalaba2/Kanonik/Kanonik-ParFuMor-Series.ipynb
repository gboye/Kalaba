{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParFuMor (version YAML & Objets)\n",
    "\n",
    "- Ajout d'un %store pour la gestion des numéros de kanoniks (16/04/20)\n",
    "\n",
    "- Attention les règles qui ne changent pas le radical donnent lieu à des mauvais découpages des formes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "import os\n",
    "from os.path import expanduser\n",
    "import itertools\n",
    "import yaml,YamlDuplicates\n",
    "from yaml.constructor import ConstructorError\n",
    "import warnings\n",
    "import ParFuMor as PFM\n",
    "from ParFuMor import *\n",
    "import pickle\n",
    "from IPython.display import HTML, display\n",
    "from cellbell import ding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestion partagée des numéros à traiter\n",
    "\n",
    "le block suivant permet de partager les numéros à traiter entre les différents Notebooks.\n",
    "- %store -r variable lit la variable dans le stock\n",
    "- %store variable stocke la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25]\n",
      "Stored 'numerosKanoniks' (list)\n"
     ]
    }
   ],
   "source": [
    "%store -r numerosKanoniks\n",
    "numerosKanoniks=[1,2,3,4,5]\n",
    "numerosKanoniks=[25]\n",
    "print numerosKanoniks\n",
    "%store numerosKanoniks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = expanduser(\"~\")\n",
    "annee=\"2020\"\n",
    "annee=\"20-Kanoniks\"\n",
    "repertoire=home+\"/ownCloud/Cours/Bordeaux/L1-UE4-Morphologie/00-Kanonik/\"+annee\n",
    "serie=repertoire+\"/\"\n",
    "nomKanoniks=[serie+\"K%02d/\"%num for num in numerosKanoniks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExemple(lDict):\n",
    "    exKey=lDict.keys()[0]\n",
    "    if isinstance(lDict[exKey],dict):\n",
    "        result=getExemple(lDict[exKey])\n",
    "    elif isinstance(lDict[exKey],list):\n",
    "        result=lDict[exKey][0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/gilles/ownCloud/Cours/Bordeaux/L1-UE4-Morphologie/00-Kanonik/20-Kanoniks/K25/\n",
      "ADJ : Genre Nombre\n",
      "NOM : Genre Nombre\n",
      "DET : Genre Nombre\n",
      "VER : Temps Pers Genre\n",
      "head stems\n",
      "head stems,VER\n",
      "head stems,NOM\n"
     ]
    }
   ],
   "source": [
    "PFM.duplicateErrors=[]\n",
    "for serie in nomKanoniks:\n",
    "    print\n",
    "    print serie\n",
    "\n",
    "    with open(serie+\"Gloses.yaml\", 'r') as stream:\n",
    "        gloses=yaml.load(stream)\n",
    "        PFM.gloses=gloses\n",
    "    with open(serie+\"Stems.yaml\", 'r') as stream:\n",
    "        try:\n",
    "            stems=yaml.load(stream)\n",
    "            PFM.stems=stems\n",
    "        except ConstructorError,msg:\n",
    "            print msg\n",
    "            continue\n",
    "    lexiqueTestPrep=getExemple(stems[\"PREP\"])\n",
    "    lexiqueTestNoun=getExemple(stems[\"NOM\"])\n",
    "    lexiqueTestHyper=getExemple(stems[\"ADJ\"])\n",
    "    with open(serie+\"Blocks.yaml\", 'r') as stream:\n",
    "        blocks=yaml.load(stream)\n",
    "        PFM.blocks=blocks\n",
    "    with open(serie+\"Phonology.yaml\", 'r') as stream:\n",
    "        phonology=yaml.load(stream)\n",
    "        PFM.phonology=phonology\n",
    "    with open(serie+\"MorphoSyntax.yaml\", 'r') as stream:\n",
    "        morphosyntax=yaml.load(stream)\n",
    "        PFM.morphosyntax=morphosyntax\n",
    "\n",
    "        \n",
    "    for cat in morphosyntax[\"Attributs\"]:\n",
    "        if sorted(morphosyntax[\"Attributs\"][cat])!=sorted(gloses[cat].keys()):\n",
    "            warnings.warn(\"\\n\"+serie+\"\\nLes attributs de %s ne sont pas cohérents\\nMorphosyntax => %s\\nGloses => %s\"%(cat,\", \".join(morphosyntax[\"Attributs\"][cat]),\", \".join(gloses[cat].keys())))\n",
    "    \n",
    "    regles=Regles()\n",
    "    PFM.regles=regles\n",
    "    for categorie in blocks:\n",
    "        regles.addBlocs(categorie,blocks[categorie])\n",
    "\n",
    "\n",
    "    paradigmes=Paradigmes()\n",
    "    PFM.paradigmes=paradigmes\n",
    "\n",
    "    hierarchieCF=HierarchieCF()\n",
    "    PFM.hierarchieCF=hierarchieCF\n",
    "    lexique=Lexique()\n",
    "    PFM.lexique=lexique\n",
    "\n",
    "\n",
    "    for cat in gloses:\n",
    "    #    print cat\n",
    "        attributes=[]\n",
    "        if gloses[cat]:\n",
    "            if set(gloses[cat].keys())==set(morphosyntax[\"Attributs\"][cat]):\n",
    "                features=morphosyntax[\"Attributs\"][cat]\n",
    "    #            print \"inhérent\",features\n",
    "            else:\n",
    "                features=gloses[cat].keys()\n",
    "    #            print \"contextuel\",features\n",
    "            for attribute in features:\n",
    "                attributes.append(gloses[cat][attribute])\n",
    "            nuplets=(itertools.product(*attributes))\n",
    "            for nuplet in nuplets:\n",
    "                proprietes=[cat]\n",
    "                for element in range(len(nuplet)):\n",
    "                    proprietes.append(u\"%s=%s\"%(features[element],nuplet[element]))\n",
    "                paradigmes.addForme(cat,proprietes)\n",
    "\n",
    "    analyserGloses(gloses)\n",
    "    analyserStems(stems)\n",
    "        \n",
    "    with open(serie+\"Hierarchie-S2.pkl\", 'wb') as output:\n",
    "       pickle.dump(hierarchieCF, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(serie+\"Lexique-S2.pkl\", 'wb') as output:\n",
    "       pickle.dump(lexique, output, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(serie+\"Regles-S2.pkl\", 'wb') as output:\n",
    "       pickle.dump(regles, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    PFM.lexique.lexemes[lexiqueTestPrep]\n",
    "\n",
    "    mot=lexiqueTestNoun\n",
    "    classesNom=PFM.lexique.formeLexeme[mot][0].split('.')[1:]\n",
    "    [classeElement for classeElement in classesNom if classeElement in gloses[\"NOM\"][\"Genre\"]]\n",
    "\n",
    "    PFM.lexique.formeLexeme[lexiqueTestHyper][0]\n",
    "\n",
    "if PFM.duplicateErrors:\n",
    "    print\n",
    "    print \"=======ERREURS========\"\n",
    "    print\n",
    "    for ligne in PFM.duplicateErrors:\n",
    "        print ligne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sur'"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getExemple(stems[\"PREP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noir\n",
      "cuisine.N\n",
      "sur\n"
     ]
    }
   ],
   "source": [
    "print PFM.lexique.formeLexeme[lexiqueTestHyper][0]\n",
    "print PFM.lexique.formeLexeme[lexiqueTestNoun][0]\n",
    "print PFM.lexique.formeLexeme[lexiqueTestPrep][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'DET, Genre=M, Nombre=Sg',\n",
       " u'DET, Genre=M, Nombre=Pl',\n",
       " u'DET, Genre=F, Nombre=Sg',\n",
       " u'DET, Genre=F, Nombre=Pl',\n",
       " u'DET, Genre=N, Nombre=Sg',\n",
       " u'DET, Genre=N, Nombre=Pl']"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paradigmes.getSigmas([\"DET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF=\n",
      "['CF=N1', 'CF=N2']\n",
      "does not apply\n"
     ]
    }
   ],
   "source": [
    "case=\"CF=N3\"\n",
    "m=re.match(ur\"(.*=)(.*)\",\"CF=N1|N2\")\n",
    "if m:\n",
    "    altTrait=m.group(1)\n",
    "    altValeurs=m.group(2).split(\"|\")\n",
    "    print altTrait\n",
    "    altTraits=[altTrait+v for v in altValeurs]\n",
    "    print altTraits\n",
    "    if any(t == case for t in altTraits):\n",
    "        print \"applies\"\n",
    "    else:\n",
    "        print \"does not apply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regles.getRules(\"NOM\",'CF=N3, Genre=C, Nombre=Du, Cas=Acc')\n",
    "regles.getRules(\"VER\",\"Pers=3Pau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Nombre</th>\n",
       "      <th>Pl</th>\n",
       "      <th>Sg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genre</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>RAD-u-ra</td>\n",
       "      <td>RAD-u-mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>RAD-a-ra</td>\n",
       "      <td>RAD-a-mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>RAD-i-ra</td>\n",
       "      <td>RAD-i-mi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Nombre        Pl        Sg\n",
       "Genre                     \n",
       "F       RAD-u-ra  RAD-u-mi\n",
       "M       RAD-a-ra  RAD-a-mi\n",
       "N       RAD-i-ra  RAD-i-mi"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=Lexeme(\"RAD\",\"DET\",\"croc\").getParadigm(lignes=[\"Genre\"],colonne=[\"Nombre\"])\n",
    "#test=Lexeme(\"RAD\",\"DET\",\"croc\").getParadigm(lignes=[\"Genre\"],colonne=[\"Nombre\",\"Cas\"])\n",
    "test\n",
    "#test.index.sortlevel(0,ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "lignes=[\"Temps\",\"Pers\",\"Trans\"]\n",
    "colonne=[\"Genre\"]\n",
    "lignes=[\"Genre\",\"Nombre\"]\n",
    "colonne=[\"Cas\"]\n",
    "for cf in [\"N%d\"%(i+1) for i in range(4)]:\n",
    "    test=Lexeme(\"RAD\",cf,\"croc\").getParadigm(lignes=lignes,colonne=colonne)\n",
    "    test=test.reset_index().groupby(by=[\"Erg\",\"Abs\",\"Dat\",\"Obl\"]).agg(lambda x: \", \".join(set(x))).reset_index().set_index(lignes)\n",
    "    print cf\n",
    "    display(test)\n",
    "\n",
    "lignes=[\"Cas\",\"Genre\"]\n",
    "colonne=[\"Nombre\"]\n",
    "for cf in [\"A%d\"%(i+1) for i in range(2)]:\n",
    "#for cf in [\"DET\"]:\n",
    "    test=Lexeme(\"RAD\",cf,\"croc\").getParadigm(lignes=lignes,colonne=colonne)\n",
    "    test=test.reset_index().groupby(by=[\"Sg\",\"Du\",\"Pl\"]).agg(lambda x: \", \".join(set(x))).reset_index().set_index(lignes)\n",
    "#    test=test.reset_index().groupby(by=[\"Sg\",\"Du\",\"Pl\"]).agg(lambda x: \", \".join(set(x))).reset_index().set_index(lignes)\n",
    "    print cf\n",
    "    display(test)\n",
    "\n",
    "lignes=[\"Genre\"]\n",
    "colonne=[\"Nombre\"]\n",
    "for cf in [\"DET\"]:\n",
    "    test=Lexeme(\"RAD\",cf,\"croc\").getParadigm(lignes=lignes,colonne=colonne)\n",
    "    test=test.reset_index().groupby(by=[\"Sg\",\"Du\",\"Pl\"]).agg(lambda x: \", \".join(set(x))).reset_index().set_index(lignes)\n",
    "    print cf\n",
    "    display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat=\"N\"\n",
    "triLex={}\n",
    "for element in PFM.lexique.lexemes:\n",
    "#    print element\n",
    "    elementElts=element.split(\".\")\n",
    "    nom=elementElts[0]\n",
    "    if len(elementElts)==3:\n",
    "        if elementElts[1].startswith(cat):\n",
    "            if not elementElts[2] in triLex:\n",
    "                triLex[elementElts[2]]=set()\n",
    "            triLex[elementElts[2]].add((PFM.lexique.lexemes[element].stem,nom))            \n",
    "triLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sauvage veliS\n",
      "boisson.F veroma\n",
      "doux sanuk\n",
      "viande.M kalo\n",
      "violet womab\n",
      "pied.F zepek\n",
      "acheter.V1 vaseb\n",
      "chat.F wili\n",
      "jambon.F tivoSa\n",
      "talentueux minag\n",
      "DEM t\n",
      "rouge Sok\n",
      "chasser.V1 moziS\n",
      "chaud dofap\n",
      "aimer.V2 lidum\n",
      "patte.N tapana\n",
      "DEF p\n",
      "lécher.V2 jogir\n",
      "boire.V1 divim\n",
      "fromage.M lebeZe\n",
      "agripper.V1 laslo\n",
      "être.V1 kotep\n",
      "roux Zikir\n",
      "table.N jolade\n",
      "avec bib\n",
      "chocolat.F ljuZu\n",
      "cuisiner.V2 tasuz\n",
      "soleil.M Sufawa\n",
      "joli taman\n",
      "gâteau.N barneto\n",
      "Félix.M feliSi\n",
      "verre.N roSa\n",
      "manger.V1 natub\n",
      "fondre.V2 peSir\n",
      "cabane.M sebi\n",
      "demander.V2 fewor\n",
      "sur af\n",
      "vers wub\n",
      "de sop\n",
      "apporter.V2 rapes\n",
      "dimanche.F deNe\n",
      "plat.N sugu\n",
      "repas.F kuse\n",
      "grand kapeS\n",
      "sauter.V2 salur\n",
      "patate.M gakeku\n",
      "vallée.F kalu\n",
      "croquette.M ovina\n",
      "cuisine.N kupa\n",
      "vieux rekod\n",
      "journée.N rapum\n",
      "noir pusis\n",
      "coquelicot.N vabo\n",
      "accompagner.V1 poZaw\n",
      "poulet.F pitamo\n",
      "préparer.V1 kroSil\n",
      "offrir.V1 vodas\n",
      "recouvrir.V2 vaniv\n",
      "à nak\n",
      "louche.N biseki\n",
      "devant lab\n",
      "enfant.F molo\n",
      "dans tep\n",
      "Martin.M martiN\n",
      "sentir.V2 guSad\n",
      "pour diS\n",
      "sous kuk\n",
      "Johana.F joana\n",
      "derrière raj\n",
      "bon pimef\n",
      "finir.V1 bibel\n",
      "IND s\n",
      "habiter.V2 mikar\n",
      "regarder.V1 dasiri\n",
      "marmite.M dusoli\n",
      "four.N Zuko\n",
      "biscuit.M tanisu\n"
     ]
    }
   ],
   "source": [
    "for l in PFM.lexique.lexemes:\n",
    "    print l,PFM.lexique.lexemes[l].stem"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temp=test.reset_index()\n",
    "temp[\"Nombre\"]=pd.Categorical(temp[\"Nombre\"],categories=[\"Sg\",\"Du\",\"Pl\"],ordered=True)\n",
    "temp.sort_values(\"Nombre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "ding()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
